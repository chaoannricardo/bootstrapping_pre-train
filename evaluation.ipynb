{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5d2b16-a9f6-4e7b-8eea-5fd08ccb393c",
   "metadata": {},
   "source": [
    "# GBN Evaluation\n",
    "# Import packages and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5557d4a0-150e-4e23-8a83-301c6726c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.model import GBNEncoder, LNClassifier, GBNDecoder\n",
    "from core.sim_metric import EDSim, SDPSim, CosSim\n",
    "from core.util import get_optimizer, get_linear_schedule_with_warmup\n",
    "from core.dataloader import load_graph\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "opt_encoder = {'data_dir': '../KnowledgeGraph_materials/data_kg/bootstrapnet_data/boot_pretrain_data_revised/',\n",
    "       'output_model_file': './models/test', \n",
    "       'input_model_file': './models/210914_fineTuned_encoder', \n",
    "       'sim_metric': CosSim(), \n",
    "        'k_hop': 1, \n",
    "       'nl_weight': 0.01, \n",
    "       'local': False, \n",
    "       'n_layer': 3, \n",
    "       'dropout': 0.1, \n",
    "       'negative_slope': 0.2, \n",
    "       'bias': False, \n",
    "       'device': torch.device(type='cuda', index=0), \n",
    "       'cpu': True,\n",
    "       'seed': 1, \n",
    "       'n_epoch': 100, \n",
    "       'optimizer': 'adam', \n",
    "       'lr': 0.001, \n",
    "       'decay': 0.001, \n",
    "       'max_grad_norm': 1.0, \n",
    "       'feature_type': 'random', \n",
    "       'feature_dim': 50, \n",
    "       'edge_feature_dim': 5}\n",
    "\n",
    "\n",
    "opt_decoder = {'dataset': '../KnowledgeGraph_materials/data_kg/bootstrapnet_data/boot_pretrain_data/gum_train/',\n",
    "               'input_model_file': './models/210914_fineTuned_decoder',\n",
    "               'output_model_file': '',\n",
    "               'method': 'multi_view',\n",
    "               'sim_metric': 'cos',\n",
    "               'n_iter': 20,\n",
    "               'min_match': 1,\n",
    "               'n_expansion': 10,\n",
    "               'k_hop': 2,\n",
    "               'un_weight': 0.01,\n",
    "               'local': False,\n",
    "               'mean_updated': False,\n",
    "               'n_layer': 3,\n",
    "               'dropout': 0.1,\n",
    "               'negative_slope': 0.2,\n",
    "               'bias': False,\n",
    "               'device': torch.device(type='cuda', index=0),\n",
    "               'cpu': True,\n",
    "               'seed': 1,\n",
    "               'init_encoder_epoch': 200,\n",
    "               'init_decoder_epoch': 200,\n",
    "               'encoder_epoch': 50,\n",
    "               'decoder_epoch': 50,\n",
    "               'optimizer': 'adam',\n",
    "               'lr': 0.001,\n",
    "               'decay': 0.001,\n",
    "               'max_grad_norm': 1.0,\n",
    "               'feature_type': 'glove',\n",
    "               'feature_dim': 50,\n",
    "               'edge_feature_dim': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0f73d-b9a1-44be-a460-0be7259c6b66",
   "metadata": {},
   "source": [
    "# Load encoder & decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80691320-680c-4365-bbfc-13d7d3cb2c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "encoder = GBNEncoder(opt_encoder)\n",
    "\n",
    "encoder.load_state_dict(torch.load(opt_encoder['input_model_file'] +'.pth'))\n",
    "\n",
    "decoder = GBNDecoder(opt_decoder, opt_decoder['sim_metric'])\n",
    "\n",
    "decoder.load_state_dict(torch.load(opt_decoder['input_model_file'] +'.pth'))\n",
    "\n",
    "# print(encoder.eval())\n",
    "# print(decoder.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f88468a-dfec-47f3-8ca0-fe7a55d6cc9f",
   "metadata": {},
   "source": [
    "# Predict links by encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "272d7471-381f-40f2-bdc4-7126a01884b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprise_data(opt, encoder, weight):\n",
    "    \n",
    "    device = \"cpu\" if opt[\"cpu\"] else opt['device']\n",
    "    \n",
    "    pkl_path = 'graph_' + opt['feature_type'] + '.pkl'\n",
    "    graph_data, graph = load_graph(opt, pkl_path, pyg=True)\n",
    "    opt['n_class'] = len(graph.node_s.itol)\n",
    "\n",
    "    graph_data = graph_data.to(device)\n",
    "    graph_data.x = (graph_data.x[0].to(device),\n",
    "                    graph_data.x[1].to(device))\n",
    "        \n",
    "    d_es = graph_data.x[0].size(-1)\n",
    "    classifier = LNClassifier(d_es * 2, 1)\n",
    "    classifier.to(device)\n",
    "    parameters = [\n",
    "        {'params': [p for p in encoder.parameters() if p.requires_grad]},\n",
    "        {'params': [p for p in classifier.parameters() if p.requires_grad]}]\n",
    "    optimizer = get_optimizer(opt['optimizer'], parameters,\n",
    "                              opt['lr'], opt['decay'])\n",
    "    n_epoch = opt['n_epoch'] * weight\n",
    "    warm_step = n_epoch * 0.1\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warm_step, n_epoch,\n",
    "                                                min_ratio=0.1)\n",
    "    print('loaded!')\n",
    "    return classifier, optimizer, scheduler, graph_data, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "487be87c-cf3d-4e3b-9912-19bf3d771b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\Ricardo\\bootstrapping_pre-train\\core\\dataloader.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_i = torch.tensor(graph.node_s.features, dtype=torch.float)\n",
      "C:\\Users\\User\\Desktop\\Ricardo\\bootstrapping_pre-train\\core\\dataloader.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_j = torch.tensor(graph.node_t.features, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop link counts: torch.Size([2, 80107])\n",
      "loaded!\n",
      "tensor([[ 0.3702],\n",
      "        [-0.8117],\n",
      "        [-0.4821],\n",
      "        ...,\n",
      "        [-0.0459],\n",
      "        [ 0.1754],\n",
      "        [ 0.3757]], grad_fn=<AddmmBackward>) tensor([[-1.0046],\n",
      "        [-0.3691],\n",
      "        [-1.1119],\n",
      "        ...,\n",
      "        [ 0.4162],\n",
      "        [-0.5518],\n",
      "        [-0.5530]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([80107, 1]) torch.Size([80104, 1])\n"
     ]
    }
   ],
   "source": [
    "def sample(high: int, size: int, device=None):\n",
    "    size = min(high, size)\n",
    "    return torch.tensor(random.sample(range(high), size), device=device)\n",
    "\n",
    "\n",
    "def _negative_sample(edge_index, size, num_neg):\n",
    "    # Handle '|V|^2 - |E| < |E|'.\n",
    "    count = size[0] * size[1]\n",
    "    num_neg = min(num_neg, count - edge_index.size(1))\n",
    "\n",
    "    row, col = edge_index\n",
    "    idx = row * size[1] + col\n",
    "\n",
    "    alpha = 1 / (1 - 1.2 * (edge_index.size(1) / count))\n",
    "\n",
    "    perm = sample(count, int(alpha * num_neg))\n",
    "    mask = torch.from_numpy(np.isin(perm, idx.to('cpu'))).to(torch.bool)\n",
    "    perm = perm[~mask][:num_neg].to(edge_index.device)\n",
    "    row = perm // size[1]\n",
    "    col = perm % size[1]\n",
    "    neg_edge_index = torch.stack([row, col], dim=0)\n",
    "    return neg_edge_index\n",
    "\n",
    "\n",
    "def edge_mask_loss(encoder_output, graph_data, masked_indice, classifier):\n",
    "    # get index of edges\n",
    "    edge_index = graph_data.edge_index\n",
    "    edge_index = edge_index[:, masked_indice]\n",
    "    size = (graph_data.x[0].size(0), graph_data.x[1].size(0))\n",
    "    neg_edge_index = _negative_sample(graph_data.edge_index, size,\n",
    "                                      num_neg=masked_indice.size(0))\n",
    "    es, ps = encoder_output\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    pos_score = classifier(torch.cat([es[edge_index[0]], ps[edge_index[1]]], dim=-1))\n",
    "    neg_score = classifier(torch.cat([es[neg_edge_index[0]], ps[neg_edge_index[1]]], dim=-1))\n",
    "    \n",
    "    print(pos_score, neg_score)\n",
    "    print(pos_score.shape, neg_score.shape)\n",
    "\n",
    "    loss = criterion(pos_score, torch.ones_like(pos_score)) + \\\n",
    "        criterion(neg_score, torch.zeros_like(neg_score))\n",
    "    loss = loss / 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def edge_mask(opt, encoder, batch, batch_id):\n",
    "    classifier, optimizer, scheduler, data, weight = batch\n",
    "    encoder.eval()\n",
    "    link_mask_pretrain(opt, encoder, classifier, data)\n",
    "\n",
    "    \n",
    "    \n",
    "def link_mask_pretrain(opt, encoder, classifier, data):\n",
    "    x = data.x\n",
    "    edge_attr = data.edge_attr\n",
    "    edge_index = data.edge_index\n",
    "    edge_size = edge_index.size(1)\n",
    "    neg_size = int(edge_size * 0.1)\n",
    "    indices = torch.randperm(edge_size, device=edge_index.device)\n",
    "    \n",
    "    output = encoder(x, edge_index[:, indices],\n",
    "                     edge_attr[indices])\n",
    "    loss = edge_mask_loss(output, data, indices, classifier)\n",
    "\n",
    "\n",
    "''' Load datasets '''\n",
    "datasets = []\n",
    "with open(os.path.join(opt_encoder['data_dir'], 'unsupervised_dataset.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        assert line and line[0]\n",
    "        dataset = line[0]\n",
    "        # setting weight to 1 when predicting\n",
    "        weight = 1\n",
    "        datasets.append((dataset, weight))\n",
    "        \n",
    "batches = []\n",
    "for dataset, weight in datasets:\n",
    "    opt_encoder['dataset'] = os.path.join(opt_encoder['data_dir'], dataset)\n",
    "    batches.append(comprise_data(opt_encoder, encoder, weight))\n",
    "\n",
    "    \n",
    "''' Begin predicting with model '''\n",
    "for i, batch in enumerate(batches):\n",
    "    # output embedding of \n",
    "    edge_mask(opt_encoder, encoder, batch, i+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7fd1a-edac-47d9-b3f9-7666330f5c33",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f690cf5d-1d7a-4114-89eb-04a4bf2a6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000, 1.5000,\n",
      "         1.5000]])\n"
     ]
    }
   ],
   "source": [
    "target = torch.ones([10, 64], dtype=torch.float32)\n",
    "output = torch.full([10, 64], 1.5)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbe8231b-3251-478a-9c1a-158f8ed56d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.313477330416026"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "  \n",
    "    z = np.exp(-x)\n",
    "    sig = 1 / (1 + z)\n",
    "\n",
    "    return sig\n",
    "\n",
    "# -np.log(sigmoid(1.5))\n",
    "-np.log(sigmoid(-4.3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
